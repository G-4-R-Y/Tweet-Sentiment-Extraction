{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Shoutouts:"},{"metadata":{},"cell_type":"markdown","source":"I would like to thank immensely the authors of many kernels which led me to fully understand the concepts needed for this submission:\n* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch#The-Model\n* https://www.kaggle.com/koushiksahu/roberta-extremely-verbosed-for-beginners\n* https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705/data?#Load-Libraries,-Data,-Tokenizer\n* https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34603010\n* https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr\n* https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert <- specially rich resource\n\nAlso, check out Abishek's YouTube channel videos, which also helped me a lot: \n* https://www.youtube.com/watch?v=6a6L_9USZxg\n* https://www.youtube.com/watch?v=XaQ0CBlQ4cY\n\nToDo: implement sentencepiece tokenizer (https://www.youtube.com/watch?v=U51ranzJBpY)"},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, LeakyReLU, Activation\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import StratifiedKFold\nimport transformers\nfrom transformers import RobertaConfig, TFRobertaModel\nimport tokenizers\n\nprint('TF version: ', tf.__version__)","execution_count":12,"outputs":[{"output_type":"stream","text":"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n/kaggle/input/tf-roberta/vocab-roberta-base.json\n/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n/kaggle/input/tf-roberta/merges-roberta-base.txt\n/kaggle/input/tf-roberta/config-roberta-base.json\nTF version:  2.2.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\nprint(train.info())","execution_count":13,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 27481 entries, 0 to 27480\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   textID         27481 non-null  object\n 1   text           27481 non-null  object\n 2   selected_text  27481 non-null  object\n 3   sentiment      27481 non-null  object\ndtypes: object(4)\nmemory usage: 858.9+ KB\nNone\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximum length\nlengths = train['text'].apply(lambda x: len(x)).tolist()\nmax(lengths)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"141"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 141\n    PAD_ID = 1\n    PATH = '../input/tf-roberta/'\n    tokenizer = tokenizers.ByteLevelBPETokenizer(\n        vocab_file = PATH+'vocab-roberta-base.json',\n        merges_file = PATH+'merges-roberta-base.txt',\n        lowercase = True,\n        add_prefix_space = True\n    )\n    sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n    n_splits = 5\n    seed = 42\n    epochs = 3\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    label_smoothing = 0.1\n    batch_size = 32","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct, config.MAX_LEN), dtype='int32')\nattention_mask = np.zeros((ct, config.MAX_LEN), dtype='int32')\ntoken_type_ids = np.zeros((ct, config.MAX_LEN), dtype='int32')\nstart_tokens = np.zeros((ct, config.MAX_LEN), dtype='int32')\nend_tokens = np.zeros((ct, config.MAX_LEN), dtype='int32')\n\nfor k in range(train.shape[0]):\n    # Selected text masking\n    text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n    text2 = \" \".join(train.loc[k, 'selected_text'].split())\n    \n    selected_idx = text1.find(text2)\n    is_selected = np.zeros((len(text1)))\n    is_selected[selected_idx:selected_idx+len(text2)] = 1\n    if text1[selected_idx-1] == \" \":\n        is_selected[selected_idx-1] = 1\n        \n    enc = config.tokenizer.encode(text1)\n    \n    # IDs start and end offsets (A.K.A.: indexes)\n    offsets = []\n    idx = 0\n    for t in enc.ids:\n        w = config.tokenizer.decode([t])\n        offsets.append((idx, idx+len(w)))\n        idx += len(w)\n        \n    # START and END tokens\n    toks = []\n    for i, (a, b) in enumerate(offsets):\n        verification_sum = np.sum(is_selected[a:b])\n        if verification_sum > 0:\n            toks.append(i)\n            \n    sentiment_tok = config.sentiment_id[train.loc[k, 'sentiment']]\n    input_ids[k, :len(enc.ids)+3] = [0, sentiment_tok] + enc.ids + [2]\n    attention_mask[k, :len(enc.ids)+3] = 1\n    if len(toks) > 0:\n        start_tokens[k, toks[0]+2] = 1\n        end_tokens[k, toks[-1]+2]  = 1","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_test = np.ones((ct, config.MAX_LEN), dtype='int32')\nattention_mask_test = np.zeros((ct, config.MAX_LEN), dtype='int32')\ntoken_type_ids_test = np.zeros((ct, config.MAX_LEN), dtype='int32')\n\nfor k in range(ct):\n    \n    # Input IDs\n    text1 = \" \" + \" \".join(test.loc[k, 'text'].split())\n    enc = config.tokenizer.encode(text1)\n    \n    sentiment_tok = config.sentiment_id[test.loc[k, 'sentiment']]\n    input_ids_test[k, :len(enc.ids)+5] = [0] + enc.ids + [2, 2] + [sentiment_tok] + [2]\n    attention_mask_test[k, :len(enc.ids)+5] = 1\n\ntest.info()\nprint(test.shape)","execution_count":17,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3534 entries, 0 to 3533\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   textID     3534 non-null   object\n 1   text       3534 non-null   object\n 2   sentiment  3534 non-null   object\ndtypes: object(3)\nmemory usage: 83.0+ KB\n(3534, 3)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"## roBERTa model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n        \ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n        \n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=config.label_smoothing)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n# from https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705/data?#Load-Libraries,-Data,-Tokenizer\n'''def build_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n\n    roberta_config = RobertaConfig.from_pretrained(config.PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(config.PATH+\n            'pretrained-roberta-base.h5',config=roberta_config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Conv1D(1,1)(x[0])\n    print(x1.shape)\n    x1 = tf.keras.layers.Flatten()(x1)\n    print(x1.shape)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    print(x1.shape)\n    \n    x2 = tf.keras.layers.Conv1D(1,1)(x[0])\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model\n'''\n\n# from https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34603010\ndef build_model():\n    ids = Input((config.MAX_LEN,), dtype=tf.int32)\n    att = Input((config.MAX_LEN,), dtype=tf.int32)\n    tok = Input((config.MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, config.PAD_ID), tf.int32)\n    \n    lens = config.MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n    \n    roberta_config = RobertaConfig.from_pretrained(config.PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(config.PATH+'pretrained-roberta-base.h5', config=roberta_config)\n    \n    x = bert_model(ids_, attention_mask=att_, token_type_ids=tok_) #for non-padded model: (ids, attention_mask=att, token_type_ids=tok)\n    #print(len(x))\n    #x = tf.convert_to_tensor(x[0])\n    #print(x.shape)\n    #print(type(x))\n    #print(type(x[0]))\n    \n    x1 = Dropout(0.15)(x[0])\n    #print(x1.shape)\n    x1 = Conv1D(768, 2, padding='same')(x1)\n    #print(x1.shape)\n    x1 = LeakyReLU()(x1)\n    #print(x1.shape)\n    x1 = Conv1D(64, 2, padding='same')(x1)\n    #print(x1.shape)\n    x1 = Dense(1)(x1)\n    #print(x1.shape)\n    x1 = Flatten()(x1)\n    #print(x1.shape)\n    x1 = Activation('softmax')(x1)\n    #print(x1.shape)\n    \n    x2 = Dropout(0.15)(x[0])\n    x2 = Conv1D(768, 2, padding='same')(x2)\n    x2 = LeakyReLU()(x2)\n    x2 = Conv1D(64, 2, padding='same')(x2)\n    x2 = Dense(1)(x2)\n    x2 = Flatten()(x2)\n    x2 = Activation('softmax')(x2)\n    \n    model = Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = Adam(learning_rate=3e-5)\n    model.compile(loss=loss_fn,\n                  optimizer=optimizer)\n  \n    x1_padded = tf.pad(x1, [[0, 0], [0, config.MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, config.MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded, x2_padded])\n    \n    return model, padded_model","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    if(len(a)==0) & (len(b)==0):\n        return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\njac = []\nVER = 'v0'\nDISPLAY = 1\n\noof_start = np.zeros((input_ids.shape[0], config.MAX_LEN))\noof_end = np.zeros((input_ids.shape[0], config.MAX_LEN))\n\npreds_start_train = np.zeros((input_ids.shape[0], config.MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0], config.MAX_LEN))\npreds_start = np.zeros((input_ids_test.shape[0], config.MAX_LEN))\npreds_end = np.zeros((input_ids_test.shape[0], config.MAX_LEN))\n\nskf = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\nfor fold, (idxText, idxSentValue) in enumerate(skf.split(input_ids, train.sentiment.values)):\n    print('\\n')\n    print('Fold', (fold+1))\n    print('\\n')\n\n    K.clear_session()\n    model, padded_model = build_model()\n\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n\n    inputText = [input_ids[idxText,], attention_mask[idxText,], token_type_ids[idxText,]]\n    targetText = [start_tokens[idxText,], end_tokens[idxText,]]\n\n    inputSentValue = [input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]]\n    targetSentValue = [start_tokens[idxSentValue,], end_tokens[idxSentValue,]]\n\n    # Sorting validation data\n    shuffleSentValue = np.int32(sorted(range(len(inputSentValue[0])), key=lambda k: (inputSentValue[0][k] == config.PAD_ID).sum(), reverse=True))\n    inputSentValue = [arr[shuffleSentValue] for arr in inputSentValue]\n    targetSentValue = [arr[shuffleSentValue] for arr in targetSentValue]\n\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    \n    for epoch in range(1, config.epochs + 1):\n        print('\\n')\n        print('Preparing data.')\n        print('\\n')\n        # add random numbers in order to avoid having the same order in each epoch\n        shuffleText = np.int32(sorted(range(len(inputText[0])), key=lambda k: (inputText[0][k] == config.PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        \n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleText) / config.batch_size)\n        batch_idxs = np.random.permutation(num_batches)\n        shuffleText_ = []\n        for batch_idx in batch_idxs:\n            shuffleText_.append(shuffleText[batch_idx * config.batch_size: (batch_idx + 1) * config.batch_size])\n        shuffleText = np.concatenate(shuffleText_)\n        \n        # reorder the input data\n        inputText = [arr[shuffleText] for arr in inputText]\n        targetText = [arr[shuffleText] for arr in targetText]\n        \n        print('\\n')\n        print('Fitting the model')\n        print('\\n')\n        #preds = padded_model.predict([input_ids_test,attention_mask_test,token_type_ids_t],verbose=DISPLAY)\n        model.fit(inputText, targetText, epochs=config.epochs, initial_epoch=epoch - 1, \n                  batch_size=config.batch_size, verbose=DISPLAY, callbacks=[], \n                  validation_data=(inputSentValue, targetSentValue), shuffle=False) #don't shuffle in fit\n        save_weights(model, weight_fn)\n        \n    print('\\n')\n    print('Loading model.')\n    print('\\n')\n    #model.load_weights('%s-roberta-%i.h5'%(VER, fold))\n    load_weights(model, weight_fn)\n\n    print('\\n')\n    print('Predicting OOF.')\n    print('\\n')\n    oof_start[idxSentValue,], oof_end[idxSentValue,] = padded_model.predict([input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]], \n                                                                            verbose=DISPLAY)\n    #oof_start[idxSentValue,], oof_end[idxSentValue,] = model.predict([input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]], \n                                                                     #verbose=DISPLAY)\n    \n    #print('\\n')\n    #print('Predicting all Train for Outlier analysis.')\n    #print('\\n')\n    #preds_train = padded_model.predict([input_ids, attention_mask, token_type_ids], verbose=DISPLAY)\n    #preds_start_train += preds_train[0] / skf.n_splits\n    #preds_end_train += preds_train[1] / skf.n_splits\n\n    print('\\n')\n    print('Predicting test data.')\n    print('\\n')\n    preds = padded_model.predict([input_ids_test, attention_mask_test, token_type_ids_test], verbose=DISPLAY)\n    #preds = model.predict([input_ids_test, attention_mask_test, token_type_ids_test], verbose=DISPLAY)\n    preds_start += preds[0] / skf.n_splits\n    preds_end += preds[1] / skf.n_splits\n\n    # display fold jaccard\n    all = []\n    for k in idxSentValue:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n\n        if a > b:\n            selected_text = train.loc[k, 'text']\n        else:\n            text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n            enc = config.tokenizer.encode(text1)\n            selected_text = config.tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(selected_text, train.loc[k, 'selected_text']))\n    jac_score = np.mean(all)\n    jac.append(jac_score)\n\n    print('\\n')\n    print('>>>>> FOLD', (fold+1), \": \\n\\tJaccard = \", jac_score)\n    print('\\n')","execution_count":20,"outputs":[{"output_type":"stream","text":"\n\nFold 1\n\n\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 1/3\n687/687 [==============================] - 105s 153ms/step - loss: 2.9230 - activation_loss: 1.4653 - activation_1_loss: 1.4577 - val_loss: 2.5994 - val_activation_loss: 1.3248 - val_activation_1_loss: 1.2746\nEpoch 2/3\n687/687 [==============================] - 101s 147ms/step - loss: 2.5527 - activation_loss: 1.2976 - activation_1_loss: 1.2551 - val_loss: 2.5773 - val_activation_loss: 1.3258 - val_activation_1_loss: 1.2515\nEpoch 3/3\n687/687 [==============================] - 102s 148ms/step - loss: 2.4123 - activation_loss: 1.2285 - activation_1_loss: 1.1838 - val_loss: 2.6041 - val_activation_loss: 1.3287 - val_activation_1_loss: 1.2755\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 2/3\n687/687 [==============================] - 102s 149ms/step - loss: 2.3109 - activation_loss: 1.1744 - activation_1_loss: 1.1365 - val_loss: 2.6230 - val_activation_loss: 1.3353 - val_activation_1_loss: 1.2877\nEpoch 3/3\n687/687 [==============================] - 102s 148ms/step - loss: 2.1630 - activation_loss: 1.0996 - activation_1_loss: 1.0634 - val_loss: 2.7278 - val_activation_loss: 1.3963 - val_activation_1_loss: 1.3315\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 3/3\n687/687 [==============================] - 101s 148ms/step - loss: 2.0442 - activation_loss: 1.0406 - activation_1_loss: 1.0035 - val_loss: 2.8306 - val_activation_loss: 1.4222 - val_activation_1_loss: 1.4083\n\n\nLoading model.\n\n\n\n\nPredicting OOF.\n\n\n172/172 [==============================] - 13s 76ms/step\n\n\nPredicting test data.\n\n\n111/111 [==============================] - 9s 79ms/step\n\n\n>>>>> FOLD 1 : \n\tJaccard =  0.6843106580436987\n\n\n\n\nFold 2\n\n\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 1/3\n688/688 [==============================] - 107s 155ms/step - loss: 2.9478 - activation_loss: 1.4798 - activation_1_loss: 1.4680 - val_loss: 2.5944 - val_activation_loss: 1.3115 - val_activation_1_loss: 1.2829\nEpoch 2/3\n688/688 [==============================] - 103s 150ms/step - loss: 2.6199 - activation_loss: 1.3222 - activation_1_loss: 1.2977 - val_loss: 2.6330 - val_activation_loss: 1.3222 - val_activation_1_loss: 1.3108\nEpoch 3/3\n688/688 [==============================] - 103s 149ms/step - loss: 2.5868 - activation_loss: 1.3098 - activation_1_loss: 1.2770 - val_loss: 2.5290 - val_activation_loss: 1.2829 - val_activation_1_loss: 1.2462\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 2/3\n688/688 [==============================] - 107s 155ms/step - loss: 2.4362 - activation_loss: 1.2354 - activation_1_loss: 1.2009 - val_loss: 2.4849 - val_activation_loss: 1.2519 - val_activation_1_loss: 1.2330\nEpoch 3/3\n688/688 [==============================] - 106s 154ms/step - loss: 2.3220 - activation_loss: 1.1857 - activation_1_loss: 1.1363 - val_loss: 2.5506 - val_activation_loss: 1.2806 - val_activation_1_loss: 1.2700\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 3/3\n688/688 [==============================] - 109s 159ms/step - loss: 2.2291 - activation_loss: 1.1369 - activation_1_loss: 1.0922 - val_loss: 2.6348 - val_activation_loss: 1.3444 - val_activation_1_loss: 1.2905\n\n\nLoading model.\n\n\n\n\nPredicting OOF.\n\n\n172/172 [==============================] - 13s 74ms/step\n\n\nPredicting test data.\n\n\n111/111 [==============================] - 8s 76ms/step\n\n\n>>>>> FOLD 2 : \n\tJaccard =  0.6988389646026277\n\n\n\n\nFold 3\n\n\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 1/3\n688/688 [==============================] - 112s 163ms/step - loss: 2.9693 - activation_loss: 1.4960 - activation_1_loss: 1.4733 - val_loss: 2.5990 - val_activation_loss: 1.3015 - val_activation_1_loss: 1.2975\nEpoch 2/3\n688/688 [==============================] - 109s 158ms/step - loss: 2.5986 - activation_loss: 1.3156 - activation_1_loss: 1.2830 - val_loss: 2.5977 - val_activation_loss: 1.3041 - val_activation_1_loss: 1.2936\nEpoch 3/3\n688/688 [==============================] - 109s 158ms/step - loss: 2.4770 - activation_loss: 1.2580 - activation_1_loss: 1.2190 - val_loss: 2.5838 - val_activation_loss: 1.3083 - val_activation_1_loss: 1.2755\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 2/3\n688/688 [==============================] - 104s 151ms/step - loss: 2.3346 - activation_loss: 1.1894 - activation_1_loss: 1.1452 - val_loss: 2.5657 - val_activation_loss: 1.3103 - val_activation_1_loss: 1.2554\nEpoch 3/3\n688/688 [==============================] - 104s 151ms/step - loss: 2.2051 - activation_loss: 1.1247 - activation_1_loss: 1.0804 - val_loss: 2.6572 - val_activation_loss: 1.3598 - val_activation_1_loss: 1.2974\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 3/3\n688/688 [==============================] - 112s 163ms/step - loss: 2.1673 - activation_loss: 1.1007 - activation_1_loss: 1.0667 - val_loss: 2.7037 - val_activation_loss: 1.3717 - val_activation_1_loss: 1.3321\n\n\nLoading model.\n\n\n\n\nPredicting OOF.\n\n\n172/172 [==============================] - 13s 76ms/step\n\n\nPredicting test data.\n\n\n111/111 [==============================] - 8s 76ms/step\n\n\n>>>>> FOLD 3 : \n\tJaccard =  0.6964850109839269\n\n\n\n\nFold 4\n\n\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 1/3\n688/688 [==============================] - 108s 157ms/step - loss: 2.9244 - activation_loss: 1.4686 - activation_1_loss: 1.4557 - val_loss: 2.5383 - val_activation_loss: 1.2962 - val_activation_1_loss: 1.2421\nEpoch 2/3\n688/688 [==============================] - 105s 152ms/step - loss: 2.5504 - activation_loss: 1.2904 - activation_1_loss: 1.2600 - val_loss: 2.5098 - val_activation_loss: 1.2953 - val_activation_1_loss: 1.2145\nEpoch 3/3\n688/688 [==============================] - 105s 153ms/step - loss: 2.4250 - activation_loss: 1.2266 - activation_1_loss: 1.1984 - val_loss: 2.5455 - val_activation_loss: 1.3062 - val_activation_1_loss: 1.2393\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 2/3\n688/688 [==============================] - 106s 154ms/step - loss: 2.3268 - activation_loss: 1.1799 - activation_1_loss: 1.1469 - val_loss: 2.6045 - val_activation_loss: 1.3201 - val_activation_1_loss: 1.2844\nEpoch 3/3\n688/688 [==============================] - 106s 154ms/step - loss: 2.1903 - activation_loss: 1.1092 - activation_1_loss: 1.0811 - val_loss: 2.6757 - val_activation_loss: 1.3830 - val_activation_1_loss: 1.2927\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 3/3\n688/688 [==============================] - 113s 165ms/step - loss: 2.1156 - activation_loss: 1.0738 - activation_1_loss: 1.0418 - val_loss: 2.7184 - val_activation_loss: 1.3975 - val_activation_1_loss: 1.3210\n\n\nLoading model.\n\n\n\n\nPredicting OOF.\n\n\n172/172 [==============================] - 13s 77ms/step\n\n\nPredicting test data.\n\n\n111/111 [==============================] - 9s 80ms/step\n\n\n>>>>> FOLD 4 : \n\tJaccard =  0.6996208810551348\n\n\n\n\nFold 5\n\n\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 1/3\n688/688 [==============================] - 113s 164ms/step - loss: 2.9363 - activation_loss: 1.4805 - activation_1_loss: 1.4558 - val_loss: 2.5445 - val_activation_loss: 1.2885 - val_activation_1_loss: 1.2560\nEpoch 2/3\n688/688 [==============================] - 109s 158ms/step - loss: 2.5755 - activation_loss: 1.3069 - activation_1_loss: 1.2686 - val_loss: 2.5288 - val_activation_loss: 1.2939 - val_activation_1_loss: 1.2349\nEpoch 3/3\n688/688 [==============================] - 109s 158ms/step - loss: 2.4488 - activation_loss: 1.2420 - activation_1_loss: 1.2067 - val_loss: 2.5467 - val_activation_loss: 1.3039 - val_activation_1_loss: 1.2428\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 2/3\n688/688 [==============================] - 111s 161ms/step - loss: 2.3562 - activation_loss: 1.1958 - activation_1_loss: 1.1604 - val_loss: 2.6369 - val_activation_loss: 1.3443 - val_activation_1_loss: 1.2926\nEpoch 3/3\n688/688 [==============================] - 110s 160ms/step - loss: 2.2280 - activation_loss: 1.1313 - activation_1_loss: 1.0967 - val_loss: 2.6989 - val_activation_loss: 1.3876 - val_activation_1_loss: 1.3112\n\n\nPreparing data.\n\n\n\n\nFitting the model\n\n\nEpoch 3/3\n688/688 [==============================] - 103s 150ms/step - loss: 2.0598 - activation_loss: 1.0473 - activation_1_loss: 1.0124 - val_loss: 2.6912 - val_activation_loss: 1.3771 - val_activation_1_loss: 1.3141\n\n\nLoading model.\n\n\n\n\nPredicting OOF.\n\n\n","name":"stdout"},{"output_type":"stream","text":"172/172 [==============================] - 13s 77ms/step\n\n\nPredicting test data.\n\n\n111/111 [==============================] - 9s 80ms/step\n\n\n>>>>> FOLD 5 : \n\tJaccard =  0.6987219218900285\n\n\nCPU times: user 36min 53s, sys: 6min 8s, total: 43min 2s\nWall time: 58min 1s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Overall 5Fold Cross-Validation Jaccard score:', jac_score)","execution_count":21,"outputs":[{"output_type":"stream","text":"Overall 5Fold Cross-Validation Jaccard score: 0.6987219218900285\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_dir = '../output/model/'\n","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kaggle submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_test.shape[0]):\n    a = np.argmax(preds_start[k, ])\n    b = np.argmax(preds_end[k, ])\n    \n    if a > b:\n        st = test.loc[k, 'text']\n    else:\n        text1 = \" \" + \" \".join(test.loc[k, 'text'].split())\n        enc = config.tokenizer.encode(text1)\n        st = config.tokenizer.decode(enc.ids[a-2:b-1])\n    \n    all.append(st)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\nprint(len(all))\ntest['selected_text'] = all\ntest[['textID', 'selected_text']].to_csv('submission.csv', index=False)\npd.set_option('max_colwidth', 60)\ntest[['textID', 'selected_text']].sample(25)","execution_count":24,"outputs":[{"output_type":"stream","text":"(3534, 3)\n3534\n","name":"stdout"},{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"          textID                                                selected_text\n2007  16a66e0b8c   if i have 5000 friend requests tomorrow, i`m blaming yo...\n1180  9f32a3213d                                                           is\n1418  4d22612bb6                                                          all\n388   2fdd2501d1                         anyone have some advice??? i need it\n113   322546026e                                                             \n1852  db331541b6                                     , but sunshine works too\n3470  0f93a7a6ff                                                          . i\n2261  6f91801792   lately i`ve been waking up with the sun in my face but ...\n3173  23d8ccab77                                    did not sleep a wink last\n1400  baf2ccb1cc                                                         away\n2753  e1dbabfca3                                       and i still recall how\n679   49ffb6b465                                 **** internet jus cut me off\n1142  0740ae9f9b  , do you realize your profile pic makes you look much ol...\n461   bad6311330                                                        going\n2342  d41721b745                                      two o`clock please come\n3269  49096c54e2                                                           my\n641   c3261537c6                                                     so bored\n152   90977e9614                                                          . i\n1786  ee89b16825   studio ghibli for this year; ponyo! http://bit.ly/whar8...\n3209  fbd7404138                                          ...ummmm nothing...\n958   3d17d4b72c                                                    re rained\n488   4a9ef1f9d8              wow, i really hope it gets better. asthma sucks\n991   fe7c9215ae   practice was a beast today. not going to see julian ton...\n2336  36589d3c2d   homework. ew fml. should i pull an all nighter? i think...\n1629  5f91766356                                                          too","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>selected_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2007</th>\n      <td>16a66e0b8c</td>\n      <td>if i have 5000 friend requests tomorrow, i`m blaming yo...</td>\n    </tr>\n    <tr>\n      <th>1180</th>\n      <td>9f32a3213d</td>\n      <td>is</td>\n    </tr>\n    <tr>\n      <th>1418</th>\n      <td>4d22612bb6</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>2fdd2501d1</td>\n      <td>anyone have some advice??? i need it</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>322546026e</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1852</th>\n      <td>db331541b6</td>\n      <td>, but sunshine works too</td>\n    </tr>\n    <tr>\n      <th>3470</th>\n      <td>0f93a7a6ff</td>\n      <td>. i</td>\n    </tr>\n    <tr>\n      <th>2261</th>\n      <td>6f91801792</td>\n      <td>lately i`ve been waking up with the sun in my face but ...</td>\n    </tr>\n    <tr>\n      <th>3173</th>\n      <td>23d8ccab77</td>\n      <td>did not sleep a wink last</td>\n    </tr>\n    <tr>\n      <th>1400</th>\n      <td>baf2ccb1cc</td>\n      <td>away</td>\n    </tr>\n    <tr>\n      <th>2753</th>\n      <td>e1dbabfca3</td>\n      <td>and i still recall how</td>\n    </tr>\n    <tr>\n      <th>679</th>\n      <td>49ffb6b465</td>\n      <td>**** internet jus cut me off</td>\n    </tr>\n    <tr>\n      <th>1142</th>\n      <td>0740ae9f9b</td>\n      <td>, do you realize your profile pic makes you look much ol...</td>\n    </tr>\n    <tr>\n      <th>461</th>\n      <td>bad6311330</td>\n      <td>going</td>\n    </tr>\n    <tr>\n      <th>2342</th>\n      <td>d41721b745</td>\n      <td>two o`clock please come</td>\n    </tr>\n    <tr>\n      <th>3269</th>\n      <td>49096c54e2</td>\n      <td>my</td>\n    </tr>\n    <tr>\n      <th>641</th>\n      <td>c3261537c6</td>\n      <td>so bored</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>90977e9614</td>\n      <td>. i</td>\n    </tr>\n    <tr>\n      <th>1786</th>\n      <td>ee89b16825</td>\n      <td>studio ghibli for this year; ponyo! http://bit.ly/whar8...</td>\n    </tr>\n    <tr>\n      <th>3209</th>\n      <td>fbd7404138</td>\n      <td>...ummmm nothing...</td>\n    </tr>\n    <tr>\n      <th>958</th>\n      <td>3d17d4b72c</td>\n      <td>re rained</td>\n    </tr>\n    <tr>\n      <th>488</th>\n      <td>4a9ef1f9d8</td>\n      <td>wow, i really hope it gets better. asthma sucks</td>\n    </tr>\n    <tr>\n      <th>991</th>\n      <td>fe7c9215ae</td>\n      <td>practice was a beast today. not going to see julian ton...</td>\n    </tr>\n    <tr>\n      <th>2336</th>\n      <td>36589d3c2d</td>\n      <td>homework. ew fml. should i pull an all nighter? i think...</td>\n    </tr>\n    <tr>\n      <th>1629</th>\n      <td>5f91766356</td>\n      <td>too</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}